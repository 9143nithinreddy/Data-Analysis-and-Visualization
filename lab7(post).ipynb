{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef50925a-6ffc-4d40-b1ce-c042ed6f0fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-squared statistics for each feature:\n",
      "                 Feature  Chi2 Score       P-value\n",
      "1                 Income  197.368421  7.836460e-45\n",
      "2            Gender_Male    3.333333  6.788915e-02\n",
      "8     City_San Francisco    1.333333  2.482131e-01\n",
      "4       Education_Master    0.888889  3.457786e-01\n",
      "7          City_New York    0.888889  3.457786e-01\n",
      "3  Education_High School    0.666667  4.142162e-01\n",
      "0                    Age    0.559361  4.545179e-01\n",
      "5          Education_PhD    0.083333  7.728300e-01\n",
      "6       City_Los Angeles    0.055556  8.136637e-01\n",
      "\n",
      "Top 3 features selected based on Chi-squared test:\n",
      "Index(['Income', 'Gender_Male', 'City_San Francisco'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample dataset\n",
    "data_collection = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Age': [25, 30, 45, 35, 40, 29, 50, 31, 38, 42],\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male'],\n",
    "    'Income': [50000, 60000, 70000, 80000, 90000, 55000, 75000, 85000, 95000, 100000],\n",
    "    'Education': ['Bachelor', 'Master', 'Bachelor', 'PhD', 'High School', 'Bachelor', 'Master', 'PhD', 'Bachelor', 'Master'],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'San Francisco', 'Los Angeles', 'Chicago', 'San Francisco', 'New York', 'Los Angeles'],\n",
    "    'Product_Purchased': ['Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes']\n",
    "})\n",
    "\n",
    "# Assume 'Product_Purchased' is the target class\n",
    "X = data_collection.drop(columns=['Product_Purchased', 'ID'])  # Features (exclude target and ID columns)\n",
    "y = data_collection['Product_Purchased']  # Target variable\n",
    "\n",
    "# Convert the target variable to numeric\n",
    "y = y.map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=[np.number])\n",
    "categorical_features = X.select_dtypes(include=[object])\n",
    "\n",
    "# Encode categorical variables\n",
    "X_encoded = pd.get_dummies(categorical_features, drop_first=True)\n",
    "\n",
    "# Combine numeric features with encoded categorical features\n",
    "X_preprocessed = pd.concat([numeric_features, X_encoded], axis=1)\n",
    "\n",
    "# Ensure all features are non-negative\n",
    "X_preprocessed = X_preprocessed.apply(lambda x: np.maximum(x, 0))\n",
    "\n",
    "# Apply chi-squared test\n",
    "chi2_selector = SelectKBest(chi2, k='all')  # Select all features initially\n",
    "X_kbest = chi2_selector.fit_transform(X_preprocessed, y)\n",
    "\n",
    "# Get scores and p-values\n",
    "chi2_scores = chi2_selector.scores_\n",
    "p_values = chi2_selector.pvalues_\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "chi2_results = pd.DataFrame({\n",
    "    'Feature': X_preprocessed.columns,\n",
    "    'Chi2 Score': chi2_scores,\n",
    "    'P-value': p_values\n",
    "})\n",
    "\n",
    "# Sort the results by chi2 score\n",
    "chi2_results_sorted = chi2_results.sort_values(by='Chi2 Score', ascending=False)\n",
    "\n",
    "# Display the top features based on chi-squared score\n",
    "print(\"Chi-squared statistics for each feature:\")\n",
    "print(chi2_results_sorted)\n",
    "\n",
    "# If you want to select the top 'n_features', set k to n_features in SelectKBest\n",
    "n_features = 3  # Specify the number of top features to select\n",
    "chi2_selector_top = SelectKBest(chi2, k=n_features)\n",
    "X_top_features = chi2_selector_top.fit_transform(X_preprocessed, y)\n",
    "\n",
    "# Display the top features selected\n",
    "selected_features = X_preprocessed.columns[chi2_selector_top.get_support()]\n",
    "print(f\"\\nTop {n_features} features selected based on Chi-squared test:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f20c795-cfb7-4be8-a184-44b989be8f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       2.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       2.0\n",
      "   macro avg       0.00      0.00      0.00       2.0\n",
      "weighted avg       0.00      0.00      0.00       2.0\n",
      "\n",
      "Selected features based on chi-squared test:\n",
      "['Age' 'Income' 'x0_Male' 'x1_High School' 'x1_Master' 'x1_PhD'\n",
      " 'x2_Los Angeles' 'x2_New York' 'x2_San Francisco']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\3yr_Odd_sem\\DAV\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "E:\\3yr_Odd_sem\\DAV\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "E:\\3yr_Odd_sem\\DAV\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "E:\\3yr_Odd_sem\\DAV\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "E:\\3yr_Odd_sem\\DAV\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "E:\\3yr_Odd_sem\\DAV\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Sample dataset\n",
    "data_collection = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Age': [25, 30, 45, 35, 40, 29, 50, 31, 38, 42],\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male'],\n",
    "    'Income': [50000, 60000, 70000, 80000, 90000, 55000, 75000, 85000, 95000, 100000],\n",
    "    'Education': ['Bachelor', 'Master', 'Bachelor', 'PhD', 'High School', 'Bachelor', 'Master', 'PhD', 'Bachelor', 'Master'],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'San Francisco', 'Los Angeles', 'Chicago', 'San Francisco', 'New York', 'Los Angeles'],\n",
    "    'Product_Purchased': ['Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes']\n",
    "})\n",
    "\n",
    "# Split data into features and target\n",
    "X = data_collection.drop(columns=['Product_Purchased', 'ID'])  # Features\n",
    "y = data_collection['Product_Purchased']  # Target\n",
    "\n",
    "# Convert target to numeric\n",
    "y = y.map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Define preprocessing for numeric and categorical data\n",
    "numeric_features = ['Age', 'Income']\n",
    "categorical_features = ['Gender', 'Education', 'City']\n",
    "\n",
    "# Pipeline for numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "    ('scaler', MinMaxScaler())  # Normalize features to be in range [0, 1]\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent value\n",
    "    ('onehot', OneHotEncoder(drop='first'))  # One-hot encode categorical variables\n",
    "])\n",
    "\n",
    "# Combine preprocessing pipelines for numeric and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Complete pipeline including feature selection and classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', SelectKBest(chi2, k='all')),  # Feature selection using chi-squared test\n",
    "    ('classifier', RandomForestClassifier())  # Classifier for demonstration\n",
    "])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display the selected features\n",
    "feature_names = (numeric_features + \n",
    "                  list(pipeline.named_steps['preprocessor']\n",
    "                       .transformers_[1][1]\n",
    "                       .named_steps['onehot']\n",
    "                       .get_feature_names_out()))\n",
    "selected_features = np.array(feature_names)[pipeline.named_steps['feature_selection'].get_support()]\n",
    "print(f\"Selected features based on chi-squared test:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b40eaa7b-2bf3-4c67-85d9-9a4d06338f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for Numerical Features:\n",
      "             Age         Income\n",
      "count  10.000000      10.000000\n",
      "mean   36.500000   76000.000000\n",
      "std     7.905694   17126.976772\n",
      "min    25.000000   50000.000000\n",
      "25%    30.250000   62500.000000\n",
      "50%    36.500000   77500.000000\n",
      "75%    41.500000   88750.000000\n",
      "max    50.000000  100000.000000\n",
      "\n",
      "Frequency Counts for Categorical Features:\n",
      "\n",
      "Education:\n",
      "Education\n",
      "Bachelor       4\n",
      "Master         3\n",
      "PhD            2\n",
      "High School    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "City:\n",
      "City\n",
      "New York         3\n",
      "Los Angeles      3\n",
      "Chicago          2\n",
      "San Francisco    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Product_Purchased:\n",
      "Product_Purchased\n",
      "Yes    6\n",
      "No     4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data_collection = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Age': [25, 30, 45, 35, 40, 29, 50, 31, 38, 42],\n",
    "    'Income': [50000, 60000, 70000, 80000, 90000, 55000, 75000, 85000, 95000, 100000],\n",
    "    'Education': ['Bachelor', 'Master', 'Bachelor', 'PhD', 'High School', 'Bachelor', 'Master', 'PhD', 'Bachelor', 'Master'],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'San Francisco', 'Los Angeles', 'Chicago', 'San Francisco', 'New York', 'Los Angeles'],\n",
    "    'Product_Purchased': ['Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes']\n",
    "})\n",
    "\n",
    "# Generate summary statistics for numerical features\n",
    "numerical_features = ['Age', 'Income']\n",
    "numerical_stats = data_collection[numerical_features].describe()\n",
    "\n",
    "# Frequency counts for categorical features\n",
    "categorical_features = ['Education', 'City', 'Product_Purchased']\n",
    "categorical_stats = {feature: data_collection[feature].value_counts() for feature in categorical_features}\n",
    "\n",
    "# Display the summary statistics\n",
    "print(\"Summary Statistics for Numerical Features:\")\n",
    "print(numerical_stats)\n",
    "print(\"\\nFrequency Counts for Categorical Features:\")\n",
    "for feature, counts in categorical_stats.items():\n",
    "    print(f\"\\n{feature}:\\n{counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847e45c-35fe-41ef-bcd9-43d4681fd5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
